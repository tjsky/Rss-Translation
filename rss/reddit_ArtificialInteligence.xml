<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>一个专门针对所有人工智能的子雷迪特。涵盖从AGI到AI初创公司的主题。无论您是研究人员，开发人员还是对AI感到好奇，都可以跳入！！！</description>
    <lastBuildDate>Sat, 23 Aug 2025 01:08:11 GMT</lastBuildDate>
    <item>
      <title>这是接下来的AI的下一步</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxnw9a/here_is_whats_next_with_ai_in_the_near_term/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  近期，我的意思是1  -  3年左右。这些都不是AI写的，因为我更喜欢自己的声音，因此请了解是否有随意的错误。 是每天使用AI，构建AI并咨询AI的人，我想认为我对我们接下来要去的地方有一个很好的了解。我不是那些认为AI会带来乌托邦的人之一，我也认为这也不会是世界末日。另外，我们不在气泡中。 为什么我们不在气泡中？好吧，人们仍在学习如何使用AI，许多人不定期使用该应用程序。这正在变化和增长，并且只会增加受欢迎程度。人们将更少搜索，并更多地依靠AI。用法只会继续增长。此外，公司现在开始了解AI是其解决方案的一部分。代理是城镇的讨论，将它们添加到产品中，内部工具只会继续使用更多的API调用和更多的代币。 我们不需要新的SOTA型号，我们需要使用我们拥有的模型。我知道很多人GPT-5是令人失望的，但是在我的咨询工作和建立代理商的经验中，GPT-4.1在实现我们大多数目标方面做得很好，地狱4.1米尼也效果很好。 GPT-5有效，但我不需要在目前不需要的模型上花费多余的钱。对于一般消费者而言，他们还不需要GPT-6，Grok 5或Gemini 3。我的意思是，当它出来时，它会很酷，但是我们需要赶上它。 我们现在需要的是计算推理。我们将越来越多地使用这些模型，我们需要计算。所有数据中心的堆积？是的，计算将派上用场。有很多充分的理由可以举办开放模型，并且许多公司和个人可能会，但是API便宜且容易，因此我不认为本地托管削减数据中心的增长。 工具/代理商将越来越重要。在克劳德（Claude），我们有项目和工件。在Grok中，我们有任务和项目。 Copilot有页面。随着我们在其中花费越来越多的时间，这些工具将出现更多。这只是开始。想象一下，与您选择的症状聊天。您相信这只是一个寒冷的头部，它建议一些冷药。现在，它也可能询问您是否希望使用Doordash从本地CV进行交付。您之前添加了该工具，因此它具有您的帐户信息。我迅速说“是的，请”，这使您建立了联系，并使您保持最新状态。您可以添加越来越多的消费工具将其集成到聊天中：Netflix，您的银行，亚马逊等。移至AI设备。你知道我们如何拥有Chromebook吗？ AI书将开始。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/maybeliterally     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxnw9a/here_is_whats_next_with_ai_in_the_near_term/</guid>
      <pubDate>Sat, 23 Aug 2025 01:00:08 GMT</pubDate>
    </item>
    <item>
      <title>明确的AI聊天机器人如何工作？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxn0bx/how_do_explicit_ai_chatbots_work/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我注意到有大量的AI供电显式聊天机器人。由于LLM的Chatgpt和Claude通常对这些东西具有非常严格的护栏，因此显式聊天机器人如何绕过它们以生成此内容？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/chickenbobx10k     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxn0bx/how_do_explicit_ai_chatbots_work/</guid>
      <pubDate>Sat, 23 Aug 2025 00:18:19 GMT</pubDate>
    </item>
    <item>
      <title>谁决定AI中的“道德”是什么……我们对情况的发展还可以吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxl1lo/who_decides_whats_ethical_in_aiand_are_we_okay/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  随着AI系统越来越影响招聘，警务，医疗保健和战争，道德护栏似乎含糊不清，由公司控制或反应性。每个人都同意伦理很重要，但是似乎没有人同意 谁的道德规范，或者谁能划定界限。 这取决于工程师吗？决策者？哲学家？科技首席执行官？选民？ 我最近就所有这一切都与AI伦理研究人员和顾问进行了长期的对话。不太了解技术本身，更多地是关于人类不舒服的问题：问责制，价值体系，治理。 真的很好奇这个社区的想法...  这集是在这里供任何更深入地挖掘的人：   https://www.youtube.com/watch?v=6c6c6c6c6c3jff6jff6u＆p   /u/u/moosesad1249     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxl1lo/who_decides_whats_ethical_in_aiand_are_we_okay/</guid>
      <pubDate>Fri, 22 Aug 2025 22:51:57 GMT</pubDate>
    </item>
    <item>
      <title>AI伦理框架如何发展以解决现实世界中的偏见？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxkbmv/how_can_ai_ethics_frameworks_evolve_to_address/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  作为对AI开发非常感兴趣的人，我一直在思考道德框架如何指导AI系统的部署，尤其是在减轻偏见方面。  就像AI Now Institute（2023）的研究一样，强调了许多当前的框架专注于理论指南，但通常在应对现实世界实施挑战方面，例如医疗保健AI中的偏见数据集或逐步审核您雇用您的算法。模型，还是解决方案更多地是关于设计这些系统的团队的多样化？  还有一个可执行性的问题，我们如何确保公司遵守这些道德规范而不会扼杀创新？ 我是从Crawford等人的论文中汲取的。 （2021）在《 AI伦理学杂志》中，这表明将技术审核与监管监督相结合的混合方法。  但是，我很好奇社区是否看到了有效的实际示例，或者是否有更好的替代方案。 请分享您的见解，并在可能的情况下得到消息来源或经验，让我们保持讨论的尊重和证据。期待向大家学习！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mxkbmv/how_can_ai_ethics_frameworks_frameworks_evolve_tovove_to_address/&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxkbmv/how_can_ai_ethics_frameworks_evolve_to_address/</guid>
      <pubDate>Fri, 22 Aug 2025 22:21:59 GMT</pubDate>
    </item>
    <item>
      <title>杰弗里·辛顿（Geoffrey Hinton）关于AI是否真正了解它在说什么的演讲</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxjohc/geoffrey_hintons_talk_on_whether_ai_truly/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   Geoffrey Hinton在今年早些时候在国际安全和道德AI协会主持的一次会议上发表了令人着迷的演讲（请在此处查看＆gt;  tl; dr：hinton辩称，chatgpt和其他LLMS的方式“理解”语言从根本上类似于人类的方式 - 具有巨大的含义。 一些关键要点：   AI的两个范式：70年来，我们拥有符号AI（逻辑/规则）与神经网络（学习）。神经网2012年以后获胜。 单词“千维乐高块”：欣顿的类比是，单词就像灵活，高维的形状，这些形状基于上下文和“握手”，并且是“握手”。通过注意机制与其他单词。理解意味着找到适合所有这些单词以将其融合在一起的正确方法。  llms不仅仅是“自动完成”：它们不存储文本或单词表。他们学习特征向量，可以通过复杂的互动来适应上下文。他们的知识就像我们的体重一样。 “幻觉”。很正常：我们做同样的事情。我们的记忆是构造的，没有检索的，因此我们一直在整理细节（并充满信心地做）。不同之处在于，我们通常会更好地知道何时我们制作东西（目前...）。 （有点）可怕的部分：数字代理可以通过复制权重/渐变来共享知识 - 数万亿位与句子中的约100位。这就是为什么GPT-4可以比任何人多得多的数千倍。提交由＆＃32; /u/orenda7     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxjohc/geoffrey_hintons_talk_on_whether_ai_truly/</guid>
      <pubDate>Fri, 22 Aug 2025 21:55:39 GMT</pubDate>
    </item>
    <item>
      <title>我很好奇，您在N8N中遇到的最后一次错误是什么？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxj21i/im_curious_what_was_the_last_error_you/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我很好奇，您在N8N中遇到的最后一个错误是什么，您是如何注意到它的，花了多少时间？   &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/u/yigitkursunn     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxj21i/im_curious_what_was_the_last_error_you/</guid>
      <pubDate>Fri, 22 Aug 2025 21:30:16 GMT</pubDate>
    </item>
    <item>
      <title>机器人的网络安全：机器人Vac在澳大利亚昆士兰州流氓</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxi8rs/cybersecurity_in_robots_a_robot_vac_goes_rogue_in/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   ai机器人中的网络安全？有时，即使是最聪明的机器人技术也可能会流氓！  昆士兰州的Dreame Tech机器人真空新闻公司（News Corp）报告说，逃脱了一个旅馆，从车道上滚下来，并在道路上跑了一口气，直到被一辆过往的汽车击中。录像很快就传播开来，使观众既有趣又令人困惑。   虽然这是一个轻松的故事，但它也突出了智能家居空间中的真正挑战：机器人真空有时会越过映射的界限，并最终处于危险的地方。尤其是Dreame，Ecovacs和Roborock等品牌的所有者都报告了偶尔出现的导航问题，设备在预期的区域徘徊，甚至推开开放的门。这可以被滥用并被黑客控制吗？  这些怪癖提出了有关AI和机器人可靠性，产品测试和安全功能的更大问题。尽管大多数失败是有趣而不是危险的，但它们仍然会给客户带来不必要的成本，并且会削弱对技术的信任。 随着自动化变得越来越普遍，确保可靠性将是关键。消费者应密切关注固件更新，利用边界设置，并考虑他们选择的品牌是否具有可靠的安全记录。目前，一个有趣的故事，但也提醒人们在日常设备中自动化和消费者安全的重要性。 您认为数据保护和网络安全保护要求在包括机器人在内的智能家居和智能办公设备方面应该是什么？在下面分享您的评论 资料来源：Ella McIlveen，“真空吸尘器在开发&#39;自己的思想&#39;&#39;之后为自由休息&#39;&#39;，新闻集团，2025年8月21日，文章： https://www.news.com.au/technology/gadgets/vacuum-cleaner-makes-a-break-for-freedom-after-developing-mind-of-its-own/news-story/971fa9936d83e993132af29c870cc71a  facebook上发生了什么的视频：&lt; href =“ https://www.facebook.com/sunshinecoastsnakecatchers/videos/robo-vacuum-went-rogue/3977447765900037/”&gt; https://www.facebook.com/sunshinecoastsnakecatchers/videos/our-robo-vacuum-went-rogue/39774477765900037/     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mxi8rs/cybersecurity_in_robots_a_robot_a_robot_vac_goes_rogue_rogue_in/”&gt; [links]       [comment]       ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxi8rs/cybersecurity_in_robots_a_robot_vac_goes_rogue_in/</guid>
      <pubDate>Fri, 22 Aug 2025 20:57:46 GMT</pubDate>
    </item>
    <item>
      <title>也许是艺术中使用的AI最被忽视的后果</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxi7g3/perhaps_the_most_overlooked_consequence_of_ai/</link>
      <description><![CDATA[The more Ai floods markets, becoming the norm, we will see a corresponding increase in people believing that all artwork, whether visual, music, writing, whatever... more and more people will adopt the attitude that all art uses ai. Not long after that, most people will simply assume all artwork is fully Ai generated.当他们说他们没有以任何方式使用AI时，规范将不信任。您是否想知道艺术家，作家，音乐家是否以任何方式使用AI？现在，想象一下，在不久的将来，有数十亿的歌曲在线上以某种次要的方式从使用AI中运行范围，以完全通过AI产生歌曲。 您如何知道真相？  很容易变得更容易。可能不会太遥远的未来，即使音乐表演者在舞台上，许多观众中的许多人都会在潜意识中相信那些表演的人正在通过AI在笔记本电脑上产生的歌曲进行伪装。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/artistic-raspberry59     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxi7g3/perhaps_the_most_overlooked_consequence_of_ai/</guid>
      <pubDate>Fri, 22 Aug 2025 20:56:13 GMT</pubDate>
    </item>
    <item>
      <title>我们对此有何看法？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxglbk/what_are_our_thoughts_on_this/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://www.technologyreview.com/2025/2025/08/08/21/11/122222222288/google-ggo/ pregy/pge/ 我不太确定该怎么想。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/myrkywood     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxglbk/what_are_our_thoughts_on_this/</guid>
      <pubDate>Fri, 22 Aug 2025 19:53:11 GMT</pubDate>
    </item>
    <item>
      <title>人工智能登录者正在变得厄运</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mx8nob/the_ai_doomers_are_getting_doomier/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   Matteo Wong：过去几年对Nate Soares和Dan Hendrycks感到恐惧，“他们俩都带领组织致力于防止AI消除人类的人性，” Matteo Wong写道。 “与其他AI末代言人一起，他们反复警告，颇具巨大的繁荣，机器人可能有一天流氓 - 带有世界末日的后果。但是在2025年，末日越来越近距离地倾斜了一定的宿命论……在4月，几个启示的研究人员都可以在4月份出版了“ ai 2027”，这是一个较长的模型，并逐渐成为一个apother的模型，并逐渐逐渐成为ai 2027的现场，又是一定的，是一定的，``AI 2027&#39;&#39;，这是一定的，``AI 2027&#39;&#39;的现场效果。 2027年，从那里扑灭了人类。 “ AI 2027”文章长达数十页，既挑剔又虚构，其中包含对行业趋势的详细分析，以及有关“ Openbrain”和“ openbrain”和“ Deepent”的极端推断，中国的间谍活动以及险恶的机器人。作者想象，在2030年中期，一个超级智能AI将用生物武器杀死人类：“大多数人在几个小时内死亡；少数幸存者（例如，在掩体中的预科，潜艇上的水手）被无人机擦伤。’ “但是，同时，与之相关的担忧是，即使聊天机器人似乎不再是聊天机器人，他们也不会让人们陷入聊天机器人的习惯，甚至不再是自我介绍的产品，即使是生成的，他们也不是在聊天中，甚至不再是生成的，即使是生成的，他们也不是在聊天中。流氓。”  阅读更多： https://theatln.tc/jj8qqs74 提交由＆＃32; /u/theatlantic     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mx8nob/the_ai_doomers_are_getting_doomier/</guid>
      <pubDate>Fri, 22 Aug 2025 14:52:37 GMT</pubDate>
    </item>
    <item>
      <title>AI接管我的学校</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mx6of3/ai_is_taking_over_my_school/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在线学校和第一周的AI使用令人恐惧。到目前为止，他们已经使用AI对我进行了评分（这做错了），AI RO写作作业，AI来生成图像（签署《独立宣言》吗？），他们为我们提供了3种不同的AI工具。然而，他们禁止学生以任何形式使用AI。我知道这是一所在线学校，所以每个老师有很多学生，但是那时候老师为什么有老师呢？您有AI进行任务，写单词，制作图像，帮助学生并进行评分。在某个时候，我希望完全AI老师。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/snapships4life     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mx6of3/ai_is_taking_over_my_school/</guid>
      <pubDate>Fri, 22 Aug 2025 13:34:08 GMT</pubDate>
    </item>
    <item>
      <title>哈维尔·米利（Javier Milei）的政府将监视与AI的社交媒体，以“预测未来犯罪”</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mwyvjn/javier_mileis_government_will_monitor_social/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mwyvjn/javier_mileis_government_will_monitor_social/</guid>
      <pubDate>Fri, 22 Aug 2025 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>AI编码不是实际编码更有用的技能</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mwwqu9/ai_coding_is_not_a_more_useful_skills_than_actual/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  似乎这些论坛完全是关于吹嘘自己的AI工作流程多么复杂的人。关于他们的上下文如何提出Claude代码的合法技能。我就像还好吗？这比在C ++中学习游戏开发或编写数据库或学习内存管理更为复杂吗？ 这等同于设置开发人员已经知道该怎么做的开发工作流程和环境。设置Claude代码是否比通过自定义配置和工作流程设置Neovim更复杂？可能不是。 那么，您知道克劳德代码是这项疯狂的新技能，从本质上讲，您只需在整个地方都有文本文件。归根结底，它只是以非确定性的方式生成了一堆代码。或者充其量只是成为一种花式的自动完成，因为您已经限制了模型如此之多，以至于无论如何您还是只要自己编码所有内容。 ，看来只有非编码器似乎只包含Vibe编码。同时，我去了与开发人员有关的论坛，并且有清理错误的恐怖故事。 这是关于LLM的事情，没人想承认： 它们是不可预测的，永远不会预测的。 这就是为什么我只能研究它们的原因。我没有用它们实际做实际的工作。因为工作需要上下文并试图使LLMS上下文意识到上游正在与上游进行战斗。 在短上下文中，窗口很难增加，因为 增加上下文窗口具有二次复杂性。它需要更多的矩阵乘法。 有优化，但它们具有稀疏注意的缺点。但是它的精度较小。  llms仅限于其数学。要通过上下文窗口绕过问题，您需要完全丢弃注意力机制 这对开发意味着什么？根据代码库的复杂性，LLM的性能越来越差。而且，您向LLM的外包代码越多，您对架构的介绍的黑匣子行为 因此，所有这些工作和“ AI技能”的范围比仅仅了解代码更糟糕。由于LLM的基本数学，情况将会变得更糟。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/golanglinuxguru1979       [links]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mwwqu9/ai_coding_is_not_a_more_useful_skills_than_actual/</guid>
      <pubDate>Fri, 22 Aug 2025 04:17:13 GMT</pubDate>
    </item>
    <item>
      <title>人工智能繁荣面临障碍 - 这就是为什么我认为重大估值更正即将接近</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mwgpah/the_ai_boom_is_facing_obstacles_heres_why_i/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mwgpah/the_ai_boom_is_facing_obstacles_heres_why_i/</guid>
      <pubDate>Thu, 21 Aug 2025 17:00:17 GMT</pubDate>
    </item>
    <item>
      <title>Zuckerberg冻结了AI在泡泡恐惧中招聘</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mw7i5e/zuckerberg_freezes_ai_hiring_amid_bubble_fears/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;      此举与梅塔报道的薪水急剧相反，最高可达最高10亿美元的顶级人才        马克·扎克伯格（Mark Zuckerberg泡沫。 这家技术巨头已经冻结了其“独特实验室”的招聘，只有AI首席Alexandr Wang必须批准的极少数例外。    阅读更多：   https://www.telegraph.co.uk/business/2025/08/21/zuckerberg-freezes-ai-hiring-amid-bubble-fears/      &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/thetelegraph   href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mw7i5e/zuckerberg_freezes_ai_ai_ai_amid_amid_amid_mid_bubble_fears/  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mw7i5e/zuckerberg_freezes_ai_ai_hiring_amid_amid_amid_amid_amid_amid_amid_amid_mid_bubble_fears/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mw7i5e/zuckerberg_freezes_ai_hiring_amid_bubble_fears/</guid>
      <pubDate>Thu, 21 Aug 2025 10:47:53 GMT</pubDate>
    </item>
    </channel>
</rss>