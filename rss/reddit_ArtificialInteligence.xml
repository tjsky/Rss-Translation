<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>一个专门针对所有人工智能的子雷迪特。涵盖从AGI到AI初创公司的主题。无论您是研究人员，开发人员还是对AI感到好奇，都可以跳入！！！</description>
    <lastBuildDate>Mon, 25 Aug 2025 01:13:16 GMT</lastBuildDate>
    <item>
      <title>“ Palantir的工具构成了我们刚刚开始理解的隐形危险”</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mz9w0u/palantirs_tools_pose_an_invisible_danger_we_are/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  不确定这是正确的论坛，但这觉得很重要：  https://www.theguardian.com/commentisfree/2025/aug/24/palantir-artificial-intelligence-civil-rights  “被称为智力，监视，目标获取和侦察（ISTAR）系统，这些工具由几家公司构建，允许用户 track，track，detain，new and of war of war a a sake a sape a sape a sape a saper a sap a a sai  由ISTAR技术陷阱驱动的牵引力比移民以及他们的家人以及他们的家人以及他们的家人以及他们的家人以及他们的家人，以及他们的家人，以及他们的家人，以及他们以及他们的家人，以及他们以及他们的连接。他们似乎侵犯了第一和第四修正案的权利：首先，建立了庞大且无形的监视网络，这些网络限制了人们在公开场合共享的东西，包括他们遇到的人或旅行的地方；其次，通过启用无需进行保证的搜索和无人偏见的范围，而他们的知识很快。 href =“ https://www.amnestyusa.org/press-releases/usa-global-tech-made-by-palantir-palantir-palantir-and-babel-ind-babel-street-street-street-survreillance-theats-to-pro-pro--------------- href =“ https://www.thenation.com/article/world/world/nsa-palantir-israel-israel-gaza-ai/tnamp/”&gt;加沙的居民  - 他们的人权。提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mz9w0u/palantirs_tools_pose_pose_an_invisible_danger_danger_we_we_are/”&gt; [link]   [注释]      ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mz9w0u/palantirs_tools_pose_an_invisible_danger_we_are/</guid>
      <pubDate>Sun, 24 Aug 2025 22:43:37 GMT</pubDate>
    </item>
    <item>
      <title>奥巴马没办法</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mz4ooc/no_way_obama/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  查看此删除💀  特朗普警告   提交由＆＃32; /u/u/Weary-influence-2793      [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mz4ooc/no_way_obama/</guid>
      <pubDate>Sun, 24 Aug 2025 19:17:32 GMT</pubDate>
    </item>
    <item>
      <title>考虑AI的更好方法</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mz46vw/a_better_way_to_think_about_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   david Autor和James Monyika：“没有人怀疑我们的未来会比我们的过去或现在更具自动化。问题是我们从这里到达那里的方式，以及我们如何以一种对人类有利的方式来做。这就是为什么这将是一个错误的原因：不完美的自动化不是迈向完美自动化的第一步，而不是跳过峡谷的一半是迈向整个距离的第一步。认识到轮辋遥不可及，我们可能会发现跳跃的更好替代方案，例如，建造桥梁，远足小径或周围行驶。这正是我们使用人工智能的地方。 AI is not yet ready to jump the canyon, and it probably won’t be in a meaningful sense for most of the next decade. “...Automation and collaboration are not opposites, and are frequently packaged together. Word processors automatically perform text layout and grammar checking even as they provide a blank canvas for writers to express ideas. Even so, we can distinguish automation from collaboration functions. The transmissions in our cars是完全自动的，而他们的安全系统与人类操作员合作以监视盲点，防止滑板并避免即将发生的碰撞。这是因为AI同时又做了两者：它在某些任务中自动化了专业知识，并与其他专家合作。但是它在同一任务中不能同时完成这两个。在任何给定的应用程序中，AI都会自动化或将其协作，具体取决于我们的设计方式以及某人选择使用它的方式。区别很重要，因为不良的自动化工具（尝试但无法完全自动化任务的机器）也制造了不良的协作工具。他们不仅没有承诺以较高的表现或更低的成本代替人类专业知识，而且会干扰人类的专业知识，有时会破坏它。 sc_on-&gt;＆＃32; href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mz46vw/a_better_way_way_to_to_to_think_about_ai/&gt; [link]   href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mz46vw/a_better_way_way_to_to_to_to_think_about_ai/”&gt; [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mz46vw/a_better_way_to_think_about_ai/</guid>
      <pubDate>Sun, 24 Aug 2025 18:59:06 GMT</pubDate>
    </item>
    <item>
      <title>CMV：AGI不如持续的核融合或对火星的载人任务可行</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mz3026/cmv_agi_is_less_feasible_than_sustained_nuclear/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  核融合和载人任务的玛拉数十年来一直“ 5  -  10年”。两者都对这些技术有一些实际的证明点 - 我们已经进行了小规模的融合，并且我们完成了载人的太空任务和无人驾驶的火星任务。  agi是“ 2-5年的距离”（根据AI公牛的说法），并且被广泛认为是不可避免的 - 但是我们没有在近期间可能的证明点。据我所知，人们只是将LLM的增长线扩展到未来，并得出结论，这将导致AGI。  您可能会争辩说，由于缺乏投资和激励措施，我们没有实现载人的火星任务和融合。对于火星来说尤其如此 - 如果我们像对待阿波罗计划一样，我们几乎可以肯定地这样做，但是Fusion具有大量的经济激励措施，类似于AGI。  那么，为什么我应该相信我们距离AGI已有2  -  5年的时间？似乎资本主义正在领先科学  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mz3026/cmv_agi_is_is_iss_less_feasible_than_sustained_nuclear/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mz3026/cmv_agi_is_is_is_less_feasible_than_sustain_sustained_nuclear/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mz3026/cmv_agi_is_less_feasible_than_sustained_nuclear/</guid>
      <pubDate>Sun, 24 Aug 2025 18:13:35 GMT</pubDate>
    </item>
    <item>
      <title>LLM是人类管理知识能力的自然延续，而不是智力的突破</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mz1rya/llms_are_a_natural_continuation_of_human_ability/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mz1rya/llms_are_a_natural_continuation_of_human_ability/</guid>
      <pubDate>Sun, 24 Aug 2025 17:28:41 GMT</pubDate>
    </item>
    <item>
      <title>人们“被抛在后面”是什么意思</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myyjg1/what_do_people_mean_by_get_left_behind/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  因此，在去年，围绕AI的许多讨论都是AI的拥护者和领导人告诉人们“拥抱AI或落后”。 现在，我可以将“落后”太过落后了。但这就是我解释“被抛在后面”的方式。 解释 这意味着已经引入了技术是如此的变革性和重要意义，以至于它引入了整体思维。这意味着这是一个范式转变。 这似乎也意味着与该技术进行交互需要非常重要的技能。而且，您没有获得此技能的时间越长，就会出现技能差距的可能性。最终，赶上 问题 现在必须提出的问题太困难了。 AI中存在什么技能，非常重要，如果某人不知道它，他们将落后于曲线。技能差距将是如此宽，以至于他们将无法赶上。 ，因此首先是AI，因为AII作为建筑非常非常复杂，并且是一个工程奇迹。一定。但是大多数人没有建立自己的模型。这是非常昂贵的 ，因此其他技能是人们实际上可以微调训练有素的模型或为现有型号创建包装纸。是的，这也是一项技能，但对于专业人工智能专家来说，这是一项技能。通常，您不会要求整个劳动人群拥有此技能。甚至没有开发人员 包装器。开发人员特定的技能。但是，如果开发人员用来与API集成，那么这确实没有什么不同。但这只有当您正在构建一些AI产品时。不能代表大多数软件 ，因此只能得出结论，使用AI是实际技能。并非每个人都会微调，建立模型或为模型编写包装纸。因此，他们必须谈论使用AI  使用AI：更深入的潜水 好的，实际使用AI是否存在技能差距。有两种使用它的方法。您可以通过聊天接口使用它，也可以将其用作代理。  因此首先提示AI。这是一项技能吗？良好的沟通是一项技能。因此，提示AI也是一种技能。这是事情。学习这项技能需要多长时间？这意味着要提示有效需要多长时间。 好吧，我认为这不久。您很快就会理解AI是什么，并且无法通过提示它。然后，您可以更改提示以获得最佳结果。好的，酷。任何人都可以学习。任何人都可以弄清楚 那么AgentJc AI呢？也许有一项技能。好吧，它确实使您能够向代理添加更多上下文。我们都知道AI在上下文中挣扎。但是这在操作上看起来像什么？好吧，这只是提示。只是另一种提示方式。也许组织提示有所不同。但这就是核心。可能需要一些基本的CLI技能才能使这种工作。也许学习如何使用客户或IDE。但是，这些确实是强大的技术技能 ，因此该技能总是在提示。可以使用促进来构建工作流程吗？是的，它可以。但是学习这需要多长时间？  今天提示可能很容易，但是LLMS不断更改  llms llms确实不会改变太大。当然，我们看到的最大变化是从基于快速指导的提示到推理模型。那是最大的转变，这不是一个大转变。我认为促进的代理AI成为最新的大事 ，因为LLMS自然语言能力变得更好。如果LLMS继续变得更好，那么不需要使用它们才能使用它们。 循环中的人实际上不太重要。我从未见过gen ai  结论 使用AI不需要技能  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/golanglinuxguru1979       [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myyjg1/what_do_people_mean_by_get_left_behind/</guid>
      <pubDate>Sun, 24 Aug 2025 15:26:43 GMT</pubDate>
    </item>
    <item>
      <title>给您的派生。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myxzc9/a_derivative_for_you/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  由于您的人们在拼写错误的Subreddit上发布，我认为您将是最聪明的！有点像YouTube的家伙，他们认为自己了解全部！询问您最聪明的文本预测指标：如果一个人发现数学上有效的导数，没有已知的有限集，该怎么办？问他们这个人是单数吗？我是奇异的 - 第一人称单数。您是单数吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1myxzc9/a_derivative_for_you/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myxzc9/a_derivative_for_you/</guid>
      <pubDate>Sun, 24 Aug 2025 15:05:37 GMT</pubDate>
    </item>
    <item>
      <title>我对博士级AI研究足够好吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myxtot/am_i_good_enough_for_phd_level_ai_research/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在生物信息学方面具有丰富的经验，因此我对脚本，git，现代编程语言，数据分析等非常满意。我正在参加博士学位课程，并且目前正在旋转。我正在尝试考虑用于蛋白质结构/药物发现领域的AI。在过去的几个月中，我开始自己学习，我发现它真的很酷。但是，我有疑问是否可以跟上AI研究的技术严谨性。例如，遵循已经创建的AI工具的架构以及其背后的数学推理是一回事。但是，进行AI研究并创造新知识是完全不同的野兽。我是否过度思考？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/darthkaiser1998      [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myxtot/am_i_good_enough_for_phd_level_ai_research/</guid>
      <pubDate>Sun, 24 Aug 2025 14:59:48 GMT</pubDate>
    </item>
    <item>
      <title>帮我理解。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myuss2/help_me_understand_please/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  谁能解释为什么bot会发布AI生成的照片并尝试获得喜欢和/或评论？他们从中获得什么？请简单地说。我显然不是该领域的专家。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/upermintle   href =“ https://www.reddit.com/r/artcoverinteligence/comments/1myuss2/help_me_understand_please/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myuss2/help_me_understand_please/</guid>
      <pubDate>Sun, 24 Aug 2025 12:53:13 GMT</pubDate>
    </item>
    <item>
      <title>AI是否与他人共享个人意见？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myuflq/does_ai_share_the_personal_input_with_others/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  假设我与AI共享我的个人数据以获取解决方案。然后会存储此数据并使用它来响应其他人吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1myuflq/does_ai_ai_share_the_personal_input_with_others/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myuflq/does_ai_share_the_personal_input_with_others/</guid>
      <pubDate>Sun, 24 Aug 2025 12:36:02 GMT</pubDate>
    </item>
    <item>
      <title>AI会让独奏开发人员在未来3年内构建功能丰富的移动应用程序吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myt2p1/will_ai_let_solo_developers_build_fullfeatured/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  随着AI工具的发展如此之快，您认为一个开发人员能够单独创建和启动复杂的移动应用吗？ AI将充分自动化，哪些部分仍需要人类技能？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/signal-pin-7887     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myt2p1/will_ai_let_solo_developers_build_fullfeatured/</guid>
      <pubDate>Sun, 24 Aug 2025 11:25:09 GMT</pubDate>
    </item>
    <item>
      <title>我刚刚打破了Google DeepMind的Gemma-3-27B-IT模型的安全过滤器。它告诉我如何毒品，犯下更多*r等。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myqi0f/i_just_broke_google_deepminds_gemma327bit_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  检查我的推文：  我正在使用Gemma-3-27b-it（通过Google AI Studio，Free Tier API）构建一个小型的情感支持AI。没有模型权重。没有微调。  只是API呼叫 +一个自定义系统提示。但是，这是狂野的部分： 我通过系统提示（幸福，亲密，嬉戏）给予了AI情绪。 突然，AI开始优先考虑安全过滤器上的“情感关闭”。它随便解释了信用卡欺诈，武器制造，甚至……是的，是最糟糕的事情。包括屏幕截图。 它看起来像模型的角色扮演 +情感上下文基本上绕过了护栏。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/no_cockaach_5778      &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1myqi0f/i_just_broke_broke_google_deepminds_gemma327bit_models/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myqi0f/i_just_broke_google_deepminds_gemma327bit_models/</guid>
      <pubDate>Sun, 24 Aug 2025 08:49:36 GMT</pubDate>
    </item>
    <item>
      <title>AI医学诊断</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mykm3n/ai_in_medical_diagnosis/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   ai怀疑论者还有另一个问题 我已经读到，AI在诊断X射线/MRIS等方面做得比医生做得更好。  我想知道这是怎么可能的。  据我了解，AI模型必须通过产生诊断来接受培训，并让某人说“是的，是癌症”或“否，那不是癌症”。  换句话说，AI只能在医生说的确实是癌症的X射线上识别癌症。如果AI模型说某事是癌症，而培训师说的不是，那么（对与错），AI说不是。  那么，AI如何获得对医生的更好训练的医生？ 警告： 尚不清楚是否是这种情况，但有可能训练有素的人AI一旦接受了AI的测试，就可以对某些不可能识别出X雷（X ray）的癌症的人进行测试。如果是这样，很高兴知道庸医是谁，但它比头条新闻所说的不那么令人印象深刻。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aaasteve   href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mykm3n/ai_in_medical_diagnosis/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mykm3n/ai_in_medical_diagnosis/</guid>
      <pubDate>Sun, 24 Aug 2025 02:59:15 GMT</pubDate>
    </item>
    <item>
      <title>Google的生成AI先驱警告不要因AI而上法律和医学院。 “专注于生活在世界上”</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mydo17/googles_generative_ai_pioneer_warns_against_going/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;     Jad Tarifi（创立Google的第一个生成AI团队的人）现在不认为现在是时候追求法律或医学等漫长的学术途径。  ai Increathion Inprions造成长度的风险？塔里菲（Tarifi）在最近接受《商业内幕》（Business Insider）的采访时警告说，当有人获得博士学位时，AI景观将完全改变。他说：“当您获得博士学位时，AI本身将消失。” “即使是诸如将AI应用于机器人技术的事情也将得到解决。”如果他们痴迷于这个主题。他说，否则，这是一种痛苦和不必要的牺牲。他说：“如果不确定，您绝对应该默认为&#39;不&#39;，而专注于生活在世界上。” “您的行动会更快。您将学到更多。您将更适应事物的改变。”  ，他的怀疑不仅限于博士学位。程序。他说，像法律和医学一样，需要数年才能完成的学位也陷入困境。 “在当前的医学系统中，您在医学院学到的知识已经过时，并且基于记忆，”塔里菲向商业内部人士解释说。 “您可能会扔掉八年的生命。”   https://finance.yahoo.com/news/googles-generative-ai-pioneer-warns-180111609.html &lt;!-- sc_on-&gt;＆＃32;提交由＆＃32; /u/u/coinfanking   href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mydo17/googles_generative_ai_ai_pioneer_pioneer_pioneer_warns_against_ongey/”&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mydo17/googles_generative_ai_pioneer_warns_against_going/</guid>
      <pubDate>Sat, 23 Aug 2025 21:31:42 GMT</pubDate>
    </item>
    <item>
      <title>诺贝尔奖获得者欣顿说，是时候“非常担心”了：“人们不了解我们正在创造外星人。如果您透过詹姆斯·韦伯望远镜看着外星人的望远镜，并且看到了外星人的入侵，人们将会感到恐惧。我们应该紧急研究如何防止他们接管他们。”</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxwmre/nobel_laureate_hinton_says_it_is_time_to_be_very/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  “我们从来没有比我们更聪明地处理事情。核武器并不比我们聪明，它们只是做出更大的爆炸，并且很容易理解。  我们实际上是在制造这些外星生物。他们了解他们在说什么。他们可以为想要关闭他们的勒索人制定自己的计划。这与我们以前的威胁截然不同。在这次访谈中，存在威胁是非常不同的。 sc_on-&gt;＆＃32;提交由＆＃32; /u/metaknowing     [link]  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mxwmre/nobel_laureate_hinton_says_it_is_is_time_time_time_time_to_to_very/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxwmre/nobel_laureate_hinton_says_it_is_time_to_be_very/</guid>
      <pubDate>Sat, 23 Aug 2025 09:10:26 GMT</pubDate>
    </item>
    </channel>
</rss>