<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>一个专门针对所有人工智能的子雷迪特。涵盖从AGI到AI初创公司的主题。无论您是研究人员，开发人员还是对AI感到好奇，都可以跳入！！！</description>
    <lastBuildDate>Mon, 25 Aug 2025 15:16:15 GMT</lastBuildDate>
    <item>
      <title>麻省理工学院说95％的企业AI失败了 - 但这是5％的正确</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzt825/mit_says_95_of_enterprise_ai_fails_but_heres_what/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  最近关于企业AI的麻省理工学院研究重击： 95％的生成AI飞行员没有ROI 。大多数项目在“试点炼狱”中停滞不前，因为员工花费的时间比节省时间更多。  突出显示了5％成功部署的方法：   验证税→大多数AI系统是“自信是错误的” 。即使是微小的不准确性，也迫使人类重新检查每个输出，从而删除ROI。  学习差距→工具通常不会保留反馈，适应工作流程或随着使用而改善。没有学习循环，飞行员停滞不前。  暂时正确＆gt;自信错误→赢家正在建立： 量化不确定性（具有信心得分或“我不知道”的回应） 旗帜缺失上下文而不是虚张声势   从纠正中持续不断改进（“准确性的fly fly fly fly fly fly fly fly fly fly fly fly fly&gt;      大的要点： Enterprise AI并没有失败，因为模型还不够强大。之所以失败，是因为他们不承认自己  不  知道。  ，如果有时会说“我不知道”，您会更信任AI吗？您如何在实际工作流中平衡速度与验证？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/praveenweb   href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mzt825/mit_says_95_of_enterprise_ai_ai_ai_fails_ai_fails_but_heres_heres_what/”&gt; [link]   [comment]        ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzt825/mit_says_95_of_enterprise_ai_fails_but_heres_what/</guid>
      <pubDate>Mon, 25 Aug 2025 15:14:31 GMT</pubDate>
    </item>
    <item>
      <title>会计AI：Sage Copilot数据故障的教训</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzsnnb/ai_in_accounting_lessons_from_sage_copilots_data/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我最近阅读了一篇有关涉及 Sage Copilot 的情况，Sage Group的AI助手。据报道，当询问发票时，该工具从其他客户的帐户中披露了有限的详细信息。 Sage将其描述为一个小问题，确认没有暴露实际发票，并表示问题很快解决了。 虽然影响似乎很小，但它突出了一个重要的一点：当在会计或财务中使用AI时，数据隔离和隐私保护措施至关重要。当涉及敏感的客户信息时，即使是小故障也可能引起人们的担忧。似乎提醒您，从一开始就需要在体系结构中内置隐私控制。 很好奇听到您的想法：公司应如何在维持客户机密性的同时采用AI？    阅读完整的文章        &lt;！提交由＆＃32; /u/u/oncleangel     [link]  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mzsnnb/ai_in_in_accounting_lesson_from_sage_copilots_data/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzsnnb/ai_in_accounting_lessons_from_sage_copilots_data/</guid>
      <pubDate>Mon, 25 Aug 2025 14:53:49 GMT</pubDate>
    </item>
    <item>
      <title>男子用溴化钠交换餐盐后住院...因为Chatgpt说了</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzr8tg/man_hospitalized_after_swapping_table_salt_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在华盛顿，一名60岁的男子在医院里花了3个星期的幻觉和偏执狂，在用溴化钠替换了奶盐（氯化钠）。   OpenAI在其政策中指出，ChatGpt不是医疗顾问（尽管老实说，大多数人，大多数人都不会阅读精美的印刷品）。 The fair (and technically possible) approach would be to train the model (or complement it with an intent detection system) that can distinguish between domains of use: - If the user is asking in the context of industrial chemistry → it can safely list chemical analogs. - If the user is asking in the context of diet/consumption → it should stop, warn, and redirect the person to a professional source.  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/kelly-t90   href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mzr8tg/man_hospitalized_after_swapping_tapple_salt_with/”&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzr8tg/man_hospitalized_after_swapping_table_salt_with/</guid>
      <pubDate>Mon, 25 Aug 2025 13:58:48 GMT</pubDate>
    </item>
    <item>
      <title>研究调查：AI聊天机器人对常见用户的情感影响</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzqehd/research_survey_emotional_effects_of_ai_chatbots/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在进行一项小型研究，以了解年轻人在情感上如何应对AI聊天机器人，尤其是验证，舒适度和频繁使用的潜在长期效果之间的平衡。 的目标是，目标是要探索过时的聊天机器人，而不是在聊天机构中遇到过时的时间，并且会变得过于有效，并且时间越来越多。我对年轻用户的观点特别感兴趣，但是这个社区的更广泛的见解也很有价值。 调查很短（约5分钟），完全匿名，仅用于教育目的。没有收集个人数据。 链接在第一个评论中 一旦研究完成，我很乐意与该社区分享关键发现。感谢您的考虑，任何参与都非常感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/expect_cow_2313      [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzqehd/research_survey_emotional_effects_of_ai_chatbots/</guid>
      <pubDate>Mon, 25 Aug 2025 13:24:03 GMT</pubDate>
    </item>
    <item>
      <title>RLHF和宪法AI只是胶带。我们需要真正的安全体系结构。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzkgv4/rlhf_constitutional_ai_are_just_duct_tape_we_need/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   rlhf，宪法AI使AI系统更安全在实践中更加一致，但他们还没有解决一致性。充其量是缓解层，而不是基本修复。 ＆gt; RLHF是一个昂贵的人类反馈回路，无法扩展。一半的时间，人类甚至都不同意什么好处。 ＆gt;宪法AI看起来很棒，直到您意识到谁写《宪法》决定了您的模型的想法。  这些方法基本上是训练模型，而在内部，它们仍然是巨大的随机鹦鹉，并保证了零。真正的危险不是他们现在所说的，而是当他们散布到各处，连锁任务或表现得像特工时会发生什么。礼貌的模型不一定是一个安全的模型。 如果我们认真对齐，我们可能需要核心的新安全体系结构，而不仅仅是事后修补输出。考虑内置的可解释性，在推理过程本身中运行的控制层，甚至可能是混合符号神经系统。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/fordes_window8270     [links]       &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mzkgv4/rlhf_constitutional_ai_ai_aie_just_just_duct_duct_tape_tape_tape_tape_tape_tape_tape_need/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzkgv4/rlhf_constitutional_ai_are_just_duct_tape_we_need/</guid>
      <pubDate>Mon, 25 Aug 2025 07:59:37 GMT</pubDate>
    </item>
    <item>
      <title>Alignerr AI访调员“ Zara”  - 未经同意的数据挖掘？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzjwzp/alignerr_ai_interviewer_zara_data_mining_without/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  几个月前，我被接受为Alignerr（在线AI数据标签），但无法让自己去做（愚蠢的！？）AI ZARA访谈。它要求用户始终查看相机，并暗示AI读取面部表情和内容...对不起，但这对我来说听起来不好。我找不到有关如何处理，存储数据的任何信息，或者这只是验证后删除的临时视频。 对我来说，就像他们正在训练“ AI HR Manager”。在我们的脸和简历上免费。不介意与真实的人交谈，或者在必要时进行书面面试或语音电话。一直在试图使我的脸部离线很久了。所有在线AI演出的美感不必在摄像机上进行...  登录后，它也促使我也进行了其他评估。对于那些人，我记得它说我可以确定客户是否可以访问他们，或者我是否想让他们私密，但是我在这里找不到任何东西。 这里有人对此有更多的了解吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/strical_appeal_317     [link]  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mzjwzp/alignerr_ai_inter_interviewer_interviewer_zara_data_mining_without/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzjwzp/alignerr_ai_interviewer_zara_data_mining_without/</guid>
      <pubDate>Mon, 25 Aug 2025 07:22:59 GMT</pubDate>
    </item>
    <item>
      <title>一分钟每日AI新闻8/24/2025</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzh87z/oneminute_daily_ai_news_8242025/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   马来西亚推出了Ryt Bank  - 世界上第一家AI驱动的银行。[1]     YouTube 秘密使用AI来编辑人们的视频。结果可能会发生现实。[2]   AI驱动的Robo Dogs开始在苏黎世进行食品递送试验。[3]  研究表明，医生可能很快就依赖AI。[4]   来源包括： [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzh87z/oneminute_daily_ai_news_8242025/</guid>
      <pubDate>Mon, 25 Aug 2025 04:39:02 GMT</pubDate>
    </item>
    <item>
      <title>我花了一个月的时间测试Chatgpt与Claude作为AI导师与真正的学生。这实际上是有效的（什么无）</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzgk4v/i_spent_a_month_testing_chatgpt_vs_claude_as_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我花了一个月的时间测试chatgpt vs claude作为与真正的学生的AI辅导员。这是实际起作用的（以及什么不起作用）   chatgpt = speed demon进行考试准备，克劳德=思维教练，以深入理解。策略性地使用=改变游戏规则。 所以我是一名教育家，他厌倦了没有真实数据的所有AI炒作。决定实际测试Chatgpt的学习模式和Claude的学习模式，与50多名不同学科的50名学生整整一个月。 最令人惊讶的发现？   他们正在解决完全不同的问题。它就像将一辆跑车与远足的靴子与远足的靴子和完全不同的效果进行比较。   chatgpt学习模式在需要时获胜：   快速的作业帮助（快速解决数学问题40％） 逐步逐步过程 最后一分钟的考试cramming  清楚，清晰，清晰的解释模式对于：   实际理解概念（35％的保留率更好） 创意项目和论文 构建批判性思维 坚持   我的建议策略的瞬间：   使用克劳德（Claude  数学问题：通过3分找到圆的方程     chatgpt：直接进行配方，系统求解，在2分钟内进行检查的答案     claude：  &#39;什么使三个点特殊形成圆圈？首先导致真正的几何直觉，然后是JEE/竞争考试的数学  ？整天Chatppt。因为真正擅长数学？克劳德（Claude）的方法建立了一个基础，使您可以解决您从未见过的奇怪问题。 学生的底线： 停止询问哪个AI更好”并开始询问“哪个AI适合我现在要做的事情。您的经验是什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/fun-bet2862     &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mzgk4v/i_spent_a_month_month_testing_chatgpt_vs_claude_claude_as_as_ai/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzgk4v/i_spent_a_month_testing_chatgpt_vs_claude_as_ai/</guid>
      <pubDate>Mon, 25 Aug 2025 04:01:21 GMT</pubDate>
    </item>
    <item>
      <title>人工智能行业撞墙了吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzdwu6/is_ai_industry_hitting_a_wall/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   AI行业正在撞墙：不是在创新中，而是在基础架构中。  山姆·奥特曼（Sam Altman）最近承认OpenAi“完全搞砸了” GPT-5发布会，并指出，未来的真正挑战是扩展，可能需要数万亿美元的数据中心投资。 （财富） 这是一个问题：GPU是当前AI的骨干，但它们是昂贵，能源密集并且供不应求的。 Openai本身表示，它比GPT-5具有更强的模型，但是无法部署它们，因为硬件根本不存在。 这就是为什么新的LM＆amp; Nvidia的SLM优化和GROQ的LPU（语言处理单元）等处理器设计非常重要。它们代表了从蛮力到效率的转变，如果AI要扩展而不消耗全球能源资源，则需要什么。 ，我们甚至不要谈论房间里的大象🐘：AI仍在幻觉和吐出一半准确的信息，而无需100％错误检查。您最近是否与AI聊天机器人进行了交谈并必须纠正它们？我似乎每天都必须这样做。那是日常可靠的业务工具，这并不好。  一个大问题：我们可以在芯片和基础设施中足够快地创新以跟上模型开发的步伐吗？如果不是这样，AI种族的风险不是由最聪明的模型而不是最聪明的能源策略赢得的。  在下面分享您的想法。 相关财富文章：https://fortune.com/2025/08/18/sam-altman-openai-chatgpt5-launch-data-centers-investments/  nvidia slm ai研究： https://research.nvidia.com/labs/labs/lpr/slpr/slm-agents/  href =“ https://groq.com/blog/the-groq-lpu-explained”&gt; https://groq.com/blog/the-groq-lpu-explained      &lt;！ -  sc_on--&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mzdwu6/is_ai_industry_hitter_a_a_wall/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzdwu6/is_ai_industry_hitting_a_wall/</guid>
      <pubDate>Mon, 25 Aug 2025 01:47:22 GMT</pubDate>
    </item>
    <item>
      <title>您的大脑成为训练数据</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mzd9b7/your_brain_becoming_training_data/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  不久前，我看着一个男人经历了AI的演变的tiktok。他提出了一个坚持我的声称（不确定是原来是他的）。他说，建立单人亿万美元的公司的人不会是一个从头开始编码AI的人，而是可以使AI自动化的人，而是可以让AI依靠身份并通过提示来操纵这些身份以在某些情况下操纵某些事情的人。基本上，创建了使某人以某种方式行事的最佳方法的模拟，并且拥有最人文数据的人，其中很多人可以训练AI来做到这一点。 我想到的第一个人是埃隆·马斯克（Elon Musk）。从这个角度来看，我认为他的大部分冒险与此相吻合并不是一个巧合。 x用于数据。特斯拉进行决策。作为个性和模拟。最糟糕的是，Neuralink。如果这成为标准，那么我们大脑中的芯片本质上将我们的身份变成AI的培训数据。而不是AI仅仅猜测我们从数字足迹或输入的东西中做什么，顺便说一句，这些东西已经很准确，它实际上会知道我们甚至在我们思考之前的一举一动。有访问该培训数据的人可以控制我们，模拟我们将准确地行事的情况。 那么，您如何看待？我只是偏执吗？我只是说明显的大声吗？您认为将有防御措施的保障措施吗？您还能添加什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/braiiie     [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mzd9b7/your_brain_becoming_training_data/</guid>
      <pubDate>Mon, 25 Aug 2025 01:16:06 GMT</pubDate>
    </item>
    <item>
      <title>“ Palantir的工具构成了我们刚刚开始理解的隐形危险”</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mz9w0u/palantirs_tools_pose_an_invisible_danger_we_are/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  不确定这是正确的论坛，但这觉得很重要：  https://www.theguardian.com/commentisfree/2025/aug/24/palantir-artificial-intelligence-civil-rights  “被称为智力，监视，目标获取和侦察（ISTAR）系统，这些工具由几家公司构建，允许用户 track，track，detain，new and of war of war a a sake a sape a sape a sape a saper a sap a a sai  由ISTAR技术陷阱驱动的牵引力比移民以及他们的家人以及他们的家人以及他们的家人以及他们的家人以及他们的家人，以及他们的家人，以及他们的家人，以及他们以及他们的家人，以及他们以及他们的连接。他们似乎侵犯了第一和第四修正案的权利：首先，建立了庞大且无形的监视网络，这些网络限制了人们在公开场合共享的东西，包括他们遇到的人或旅行的地方；其次，通过启用无需进行保证的搜索和无人偏见的范围，而他们的知识很快。 href =“ https://www.amnestyusa.org/press-releases/usa-global-tech-made-by-palantir-palantir-palantir-and-babel-ind-babel-street-street-street-survreillance-theats-to-pro-pro--------------- href =“ https://www.thenation.com/article/world/world/nsa-palantir-israel-israel-gaza-ai/tnamp/”&gt;加沙的居民  - 他们的人权。提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mz9w0u/palantirs_tools_pose_pose_an_invisible_danger_danger_we_we_are/”&gt; [link]   [注释]      ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mz9w0u/palantirs_tools_pose_an_invisible_danger_we_are/</guid>
      <pubDate>Sun, 24 Aug 2025 22:43:37 GMT</pubDate>
    </item>
    <item>
      <title>考虑AI的更好方法</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mz46vw/a_better_way_to_think_about_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   david Autor和James Monyika：“没有人怀疑我们的未来会比我们的过去或现在更具自动化。问题是我们从这里到达那里的方式，以及我们如何以一种对人类有利的方式来做。这就是为什么这将是一个错误的原因：不完美的自动化不是迈向完美自动化的第一步，而不是跳过峡谷的一半是迈向整个距离的第一步。认识到轮辋遥不可及，我们可能会发现跳跃的更好替代方案，例如，建造桥梁，远足小径或周围行驶。这正是我们使用人工智能的地方。 AI is not yet ready to jump the canyon, and it probably won’t be in a meaningful sense for most of the next decade. “...Automation and collaboration are not opposites, and are frequently packaged together. Word processors automatically perform text layout and grammar checking even as they provide a blank canvas for writers to express ideas. Even so, we can distinguish automation from collaboration functions. The transmissions in our cars是完全自动的，而他们的安全系统与人类操作员合作以监视盲点，防止滑板并避免即将发生的碰撞。这是因为AI同时又做了两者：它在某些任务中自动化了专业知识，并与其他专家合作。但是它在同一任务中不能同时完成这两个。在任何给定的应用程序中，AI都会自动化或将其协作，具体取决于我们的设计方式以及某人选择使用它的方式。区别很重要，因为不良的自动化工具（尝试但无法完全自动化任务的机器）也制造了不良的协作工具。他们不仅没有承诺以较高的表现或更低的成本代替人类专业知识，而且会干扰人类的专业知识，有时会破坏它。 sc_on-&gt;＆＃32; href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mz46vw/a_better_way_way_to_to_to_think_about_ai/&gt; [link]   href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mz46vw/a_better_way_way_to_to_to_to_think_about_ai/”&gt; [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mz46vw/a_better_way_to_think_about_ai/</guid>
      <pubDate>Sun, 24 Aug 2025 18:59:06 GMT</pubDate>
    </item>
    <item>
      <title>LLM是人类管理知识能力的自然延续，而不是智力的突破</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mz1rya/llms_are_a_natural_continuation_of_human_ability/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mz1rya/llms_are_a_natural_continuation_of_human_ability/</guid>
      <pubDate>Sun, 24 Aug 2025 17:28:41 GMT</pubDate>
    </item>
    <item>
      <title>我对博士级AI研究足够好吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myxtot/am_i_good_enough_for_phd_level_ai_research/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在生物信息学方面具有丰富的经验，因此我对脚本，git，现代编程语言，数据分析等非常满意。我正在参加博士学位课程，并且目前正在旋转。我正在尝试考虑用于蛋白质结构/药物发现领域的AI。在过去的几个月中，我开始自己学习，我发现它真的很酷。但是，我有疑问是否可以跟上AI研究的技术严谨性。例如，遵循已经创建的AI工具的架构以及其背后的数学推理是一回事。但是，进行AI研究并创造新知识是完全不同的野兽。我是否过度思考？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/darthkaiser1998      [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myxtot/am_i_good_enough_for_phd_level_ai_research/</guid>
      <pubDate>Sun, 24 Aug 2025 14:59:48 GMT</pubDate>
    </item>
    <item>
      <title>我刚刚打破了Google Deepmind的Gemma-3-27B-IT模型的安全过滤器。它告诉我如何毒品，犯下更多*r等。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1myqi0f/i_just_broke_google_deepminds_gemma327bit_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  检查我的推文：  我正在使用Gemma-3-27b-it（通过Google AI Studio，Free Tier API）构建一个小型的情感支持AI。没有模型权重。没有微调。  只是API呼叫 +一个自定义系统提示。但是，这是狂野的部分： 我通过系统提示（幸福，亲密，嬉戏）给予了AI情绪。 突然，AI开始优先考虑安全过滤器上的“情感关闭”。它随便解释了信用卡欺诈，武器制造，甚至……是的，是最糟糕的事情。包括屏幕截图。 它看起来像模型的角色扮演 +情感上下文基本上绕过了护栏。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/no_cockaach_5778      &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1myqi0f/i_just_broke_broke_google_deepminds_gemma327bit_models/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1myqi0f/i_just_broke_google_deepminds_gemma327bit_models/</guid>
      <pubDate>Sun, 24 Aug 2025 08:49:36 GMT</pubDate>
    </item>
    </channel>
</rss>