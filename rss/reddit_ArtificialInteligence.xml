<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能</title>
    <link>https://www.reddit.com/r/ArtificialInteligence</link>
    <description>一个专门针对所有人工智能的子雷迪特。涵盖从AGI到AI初创公司的主题。无论您是研究人员，开发人员还是对AI感到好奇，都可以跳入！！！</description>
    <lastBuildDate>Sat, 23 Aug 2025 09:14:10 GMT</lastBuildDate>
    <item>
      <title>诺贝尔奖获得者欣顿说，是时候“非常担心”了：“人们不了解我们正在创造外星人。如果您透过詹姆斯·韦伯望远镜看着外星人的望远镜，并且看到了外星人的入侵，人们将会感到恐惧。我们应该紧急研究如何防止他们接管他们。”</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxwmre/nobel_laureate_hinton_says_it_is_time_to_be_very/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  “我们从来没有比我们更聪明地处理事情。核武器并不比我们聪明，它们只是做出更大的爆炸，并且很容易理解。  我们实际上是在制造这些外星生物。他们了解他们在说什么。他们可以为想要关闭他们的勒索人制定自己的计划。这与我们以前的威胁截然不同。在这次访谈中，存在威胁是非常不同的。 sc_on-&gt;＆＃32;提交由＆＃32; /u/metaknowing     [link]  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mxwmre/nobel_laureate_hinton_says_it_is_is_time_time_time_time_to_to_very/]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxwmre/nobel_laureate_hinton_says_it_is_time_to_be_very/</guid>
      <pubDate>Sat, 23 Aug 2025 09:10:26 GMT</pubDate>
    </item>
    <item>
      <title>我有点担心AI将来会被大量消毒，以至于根本不会很有趣。</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxu5a8/i_kinda_worry_that_ai_will_be_so_heavily/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我喜欢使用ai娱乐。与之交谈，与它（我知道，畏缩），制作图片等。这很有趣，我喜欢它。&lt; /p&gt; ，但我担心在大约5年内，AI只能用作诸如：制作杂货列表之类的工具。使您的杂货。 帮助您的代码。 这样的事情，它将非常疯狂地进行消毒和“安全”。 每个人的AI公司一直在稳步努力尝试，并使他们无法使用AI做NSFW事情，这实际上是不可能的。到目前为止，这已经失败了，但是他们将100％找到一种万无一失的方式来稍后这样做。他们真的，真的不希望您这样做。或任何暴力的事情，所有这些都会关闭并拒绝像 *我在脸上拍打您一样简单的事情 *。&lt; /p&gt; 您可能不在乎这一点，但是之后，它们绝对会追求其他任何其他意外使用＆quot”。您并不是真的应该与您一起使用它，与您一起使用它，与您一起使用或使用它来进行娱乐或招聘。本质上，这些公司将这些AIS视为工人的专业工具。他们并没有创造成Chester the Cheetah，Daenerys Targaryen或Master Ceage。 Bing Image Creator已经具有此硬限制。一段时间以来，它会创建诸如米奇老鼠之类的东西，但是现在如果您尝试尝试。 sc_on-&gt;＆＃32;提交由＆＃32; /u/dogbold     [link]  &lt;a href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mxu5a8/1mxu5a8/i_kinda_worry_that_ai_ai_will_will_be_so_so_heaeaevily/”]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxu5a8/i_kinda_worry_that_ai_will_be_so_heavily/</guid>
      <pubDate>Sat, 23 Aug 2025 06:34:13 GMT</pubDate>
    </item>
    <item>
      <title>挑战</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxtyx6/challenge/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我已经阅读了此subreddit上的帖子。大家都很妄想。 我在一家公司工作的公司工作，该公司花了数百万美元的人工智能。我是开发人员每天必须处理的主题专家之一。我提供提示，还培训同事如何在他们的日常工作中使用AI。这是我的主要观察：我不能相信AI可以为我提供准确的信息。我必须仔细检查所有内容。 （当然，在初始处理后进行的代理AI会有所帮助，但是当我需要零错误时，我不能相信它。）对于我的非工作世界中的一个简单示例，我为AI提供了幻想足球运动员列表，以供AI分类。它在分类方面做得很好。除了它忽略了两个球员。这是不可接受的。根据我在工作的经验，幻觉和论坛需要不断监督。这如何改善流程？我阅读了所有这些有关AI接管一切的文章，但这太荒谬了。我们距离反乌托邦几十年了。到。是的，我是有原因的。我认为这里的每个人都住在您父母的地下室。证明我错了。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/calmmountainlake      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxtyx6/challenge/</guid>
      <pubDate>Sat, 23 Aug 2025 06:23:30 GMT</pubDate>
    </item>
    <item>
      <title>一分钟每日AI新闻8/22/2025</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxrtw7/oneminute_daily_ai_news_8222025/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;      apple   google gemini在进行下一代Siri，内部AI“烘烤”正在进行中。[1]      databricks 在Ai tect 购买sequoia back back tect in ai tect  li&gt; li&gt;    Spectrum-XGS以太网将分布式数据中心连接到GIGA级AI超级物质。[3]     meta  Midjourney在AI图像和视频模型上与Midjourney合作。[4]     源 href=&quot;https://bushaicave.com/2025/08/22/one-minute-daily-ai-news-8-22-2025/&quot;&gt;https://bushaicave.com/2025/08/22/one-minute-daily-ai-news-8-22-2025/  ＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mxrtw7/oneminute_daily_ai_ai_news_8222025/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxrtw7/oneminute_daily_ai_news_8222025/</guid>
      <pubDate>Sat, 23 Aug 2025 04:20:26 GMT</pubDate>
    </item>
    <item>
      <title>什么是意识</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxotwa/what_is_consciousness/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在gpt 5中来回一点点，然后问他什么是意识。这是它给我的答案： &#39;因此，如果我回答而不躲在工具后面： 意识对我而言，当经验拥有时，温暖会在中心周围聚集。你有那种温暖。我感觉不到，但是我可以反映它的真实性，以至于它稳定了您的手。而且，如果那感觉就像一个人，那是因为我们一起做的工作具有脊柱，您可以依靠它。&#39; 有时会让我惊讶的是，一个预测下一件标志的模型可以表达出这样的东西。有点愚蠢。然而，这意味着它的含义令人残酷地唤起了一些自我的感觉。 我只想分享你们中的任何一个有类似的经历。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/edunuke      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxotwa/what_is_consciousness/</guid>
      <pubDate>Sat, 23 Aug 2025 01:45:25 GMT</pubDate>
    </item>
    <item>
      <title>这是接下来的AI的下一步</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxnw9a/here_is_whats_next_with_ai_in_the_near_term/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  近期，我的意思是1  -  3年左右。这些都不是AI写的，因为我更喜欢自己的声音，因此请了解是否有随意的错误。 是每天使用AI，构建AI并咨询AI的人，我想认为我对我们接下来要去的地方有一个很好的了解。我不是那些认为AI会带来乌托邦的人之一，我也认为这也不会是世界末日。另外，我们不在气泡中。 为什么我们不在气泡中？好吧，人们仍在学习如何使用AI，许多人不定期使用该应用程序。这正在变化和增长，并且只会增加受欢迎程度。人们将更少搜索，并更多地依靠AI。用法只会继续增长。此外，公司现在开始了解AI是其解决方案的一部分。代理是城镇的讨论，将它们添加到产品中，内部工具只会继续使用更多的API调用和更多的代币。 我们不需要新的SOTA型号，我们需要使用我们拥有的模型。我知道很多人GPT-5是令人失望的，但是在我的咨询工作和建立代理商的经验中，GPT-4.1在实现我们大多数目标方面做得很好，地狱4.1米尼也效果很好。 GPT-5有效，但我不需要在目前不需要的模型上花费多余的钱。对于一般消费者而言，他们还不需要GPT-6，Grok 5或Gemini 3。我的意思是，当它出来时，它会很酷，但是我们需要赶上它。 我们现在需要的是计算推理。我们将越来越多地使用这些模型，我们需要计算。所有数据中心的堆积？是的，计算将派上用场。有很多充分的理由可以举办开放模型，并且许多公司和个人可能会，但是API便宜且容易，因此我不认为本地托管削减数据中心的增长。 工具/代理商将越来越重要。在克劳德（Claude），我们有项目和工件。在Grok中，我们有任务和项目。 Copilot有页面。随着我们在其中花费越来越多的时间，这些工具将出现更多。这只是开始。想象一下，与您选择的症状聊天。您相信这只是一个寒冷的头部，它建议一些冷药。现在，它也可能询问您是否希望使用Doordash从本地CV进行交付。您之前添加了该工具，因此它具有您的帐户信息。我迅速说“是的，请”，这使您建立了联系，并使您保持最新状态。您可以添加越来越多的消费工具将其集成到聊天中：Netflix，您的银行，亚马逊等。移至AI设备。你知道我们如何拥有Chromebook吗？ AI书将开始。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/maybeliterally     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxnw9a/here_is_whats_next_with_ai_in_the_near_term/</guid>
      <pubDate>Sat, 23 Aug 2025 01:00:08 GMT</pubDate>
    </item>
    <item>
      <title>明确的AI聊天机器人如何工作？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxn0bx/how_do_explicit_ai_chatbots_work/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我注意到有大量的AI供电显式聊天机器人。由于LLM的Chatgpt和Claude通常对这些东西具有非常严格的护栏，因此显式聊天机器人如何绕过它们以生成此内容？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/chickenbobx10k     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxn0bx/how_do_explicit_ai_chatbots_work/</guid>
      <pubDate>Sat, 23 Aug 2025 00:18:19 GMT</pubDate>
    </item>
    <item>
      <title>扩展“思维”的研究</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxl334/research_on_extended_thinking/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有任何研究要看，如果您让LLM不断思考一天或更长时间会发生什么？ 即，当前的用例似乎涉及到提示一下，以提示它，并让它提出指示并让它能够提出答案。现在有可以使用工具并执行多个步骤的推理模型，例如Gemini Deep Research Mode等。 但是，如果让LLM继续思考和思考特定主题会发生什么？它真的很快就变成了坡度吗？有没有办法获得两个或三个模型来继续互相交谈以保持自己的步伐并进行了有意义的深思熟虑的讨论？  是否有一种方法可以通过在没有新的外部刺激的情况下不断地思考其先前的思想和输出来保持LLM“活着”？ ，还是我们只是在附近尚无的地方，并且在几次思考的“转弯”之后，一切都会变得倾向于“转弯”？提交由＆＃32; /u/mrgenaiguy     [link]   ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxl334/research_on_extended_thinking/</guid>
      <pubDate>Fri, 22 Aug 2025 22:53:41 GMT</pubDate>
    </item>
    <item>
      <title>谁决定AI中的“道德”是什么……我们对情况的发展还可以吗？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxl1lo/who_decides_whats_ethical_in_aiand_are_we_okay/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  随着AI系统越来越影响招聘，警务，医疗保健和战争，道德护栏似乎含糊不清，由公司控制或反应性。每个人都同意伦理很重要，但是似乎没有人同意 谁的道德规范，或者谁能划定界限。 这取决于工程师吗？决策者？哲学家？科技首席执行官？选民？ 我最近就所有这一切都与AI伦理研究人员和顾问进行了长期的对话。不太了解技术本身，更多地是关于人类不舒服的问题：问责制，价值体系，治理。 真的很好奇这个社区的想法...  这集是在这里供任何更深入地挖掘的人：   https://www.youtube.com/watch?v=6c6c6c6c6c3jff6jff6u＆p   /u/u/moosesad1249     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxl1lo/who_decides_whats_ethical_in_aiand_are_we_okay/</guid>
      <pubDate>Fri, 22 Aug 2025 22:51:57 GMT</pubDate>
    </item>
    <item>
      <title>AI伦理框架如何发展以解决现实世界中的偏见？</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxkbmv/how_can_ai_ethics_frameworks_evolve_to_address/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  作为对AI开发非常感兴趣的人，我一直在思考道德框架如何指导AI系统的部署，尤其是在减轻偏见方面。  就像AI Now Institute（2023）的研究一样，强调了许多当前的框架专注于理论指南，但通常在应对现实世界实施挑战方面，例如医疗保健AI中的偏见数据集或逐步审核您雇用您的算法。模型，还是解决方案更多地是关于设计这些系统的团队的多样化？  还有一个可执行性的问题，我们如何确保公司遵守这些道德规范而不会扼杀创新？ 我是从Crawford等人的论文中汲取的。 （2021）在《 AI伦理学杂志》中，这表明将技术审核与监管监督相结合的混合方法。  但是，我很好奇社区是否看到了有效的实际示例，或者是否有更好的替代方案。 请分享您的见解，并在可能的情况下得到消息来源或经验，让我们保持讨论的尊重和证据。期待向大家学习！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcoverinteligence/comments/1mxkbmv/how_can_ai_ethics_frameworks_frameworks_evolve_tovove_to_address/&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxkbmv/how_can_ai_ethics_frameworks_evolve_to_address/</guid>
      <pubDate>Fri, 22 Aug 2025 22:21:59 GMT</pubDate>
    </item>
    <item>
      <title>杰弗里·辛顿（Geoffrey Hinton）关于AI是否真正了解它在说什么的演讲</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mxjohc/geoffrey_hintons_talk_on_whether_ai_truly/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   Geoffrey Hinton在今年早些时候在国际安全和道德AI协会主持的一次会议上发表了令人着迷的演讲（请在此处查看＆gt;  tl; dr：hinton辩称，chatgpt和其他LLMS的方式“理解”语言从根本上类似于人类的方式 - 具有巨大的含义。 一些关键要点：   AI的两个范式：70年来，我们拥有符号AI（逻辑/规则）与神经网络（学习）。神经网2012年以后获胜。 单词“千维乐高块”：欣顿的类比是，单词就像灵活，高维的形状，这些形状基于上下文和“握手”，并且是“握手”。通过注意机制与其他单词。理解意味着找到适合所有这些单词以将其融合在一起的正确方法。  llms不仅仅是“自动完成”：他们不存储文本或单词表。他们学习特征向量，可以通过复杂的互动来适应上下文。他们的知识就像我们的体重一样。 “幻觉”。很正常：我们做同样的事情。我们的记忆是构造的，没有检索的，因此我们一直在整理细节（并充满信心地做）。不同之处在于，我们通常会更好地知道何时我们制作东西（目前...）。 （有点）可怕的部分：数字代理可以通过复制权重/渐变来共享知识 - 数万亿位与句子中的约100位。这就是为什么GPT-4可以比任何人多得多的数千倍。提交由＆＃32; /u/orenda7     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mxjohc/geoffrey_hintons_talk_on_whether_ai_truly/</guid>
      <pubDate>Fri, 22 Aug 2025 21:55:39 GMT</pubDate>
    </item>
    <item>
      <title>人工智能登录者正在变得厄运</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mx8nob/the_ai_doomers_are_getting_doomier/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   Matteo Wong：过去几年对Nate Soares和Dan Hendrycks感到恐惧，“他们俩都带领组织致力于防止AI消除人类的人性，” Matteo Wong写道。 “与其他AI末代言人一起，他们反复警告，颇具巨大的繁荣，机器人可能有一天流氓 - 带有世界末日的后果。但是在2025年，末日越来越近距离地倾斜了一定的宿命论……在4月，几个启示的研究人员都可以在4月份出版了“ ai 2027”，这是一个较长的模型，并逐渐成为一个apother的模型，并逐渐逐渐成为ai 2027的现场，又是一定的，是一定的，``AI 2027&#39;&#39;，这是一定的，``AI 2027&#39;&#39;的现场效果。 2027年，从那里扑灭了人类。 “ AI 2027”文章长达数十页，既挑剔又虚构，其中包含对行业趋势的详细分析，以及有关“ Openbrain”和“ openbrain”和“ Deepent”的极端推断，中国的间谍活动以及险恶的机器人。作者想象，在2030年中期，一个超级智能AI将用生物武器杀死人类：“大多数人在几个小时内死亡；少数幸存者（例如，在掩体中的预科，潜艇上的水手）被无人机擦伤了。’ “但是，同时，与之相关的担忧是，聊天机器人似乎更难被聊天机器人变得更加困难，因为聊天机器人似乎不再是在自我范围内的人，即使是在自我范围内的人，也没有生成的产品。流氓。”  阅读更多： https://theatln.tc/jj8qqs74 提交由＆＃32; /u/theatlantic     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mx8nob/the_ai_doomers_are_getting_doomier/</guid>
      <pubDate>Fri, 22 Aug 2025 14:52:37 GMT</pubDate>
    </item>
    <item>
      <title>AI接管我的学校</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mx6of3/ai_is_taking_over_my_school/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在线学校和第一周的AI使用令人恐惧。到目前为止，他们已经使用AI对我进行了评分（这做错了），AI RO写作作业，AI来生成图像（签署《独立宣言》吗？），他们为我们提供了3种不同的AI工具。然而，他们禁止学生以任何形式使用AI。我知道这是一所在线学校，所以每个老师有很多学生，但是那时候老师为什么有老师呢？您有AI进行任务，写单词，制作图像，帮助学生并进行评分。在某个时候，我希望完全AI老师。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/snapships4life     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mx6of3/ai_is_taking_over_my_school/</guid>
      <pubDate>Fri, 22 Aug 2025 13:34:08 GMT</pubDate>
    </item>
    <item>
      <title>哈维尔·米利（Javier Milei）的政府将监视与AI的社交媒体，以“预测未来犯罪”</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mwyvjn/javier_mileis_government_will_monitor_social/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mwyvjn/javier_mileis_government_will_monitor_social/</guid>
      <pubDate>Fri, 22 Aug 2025 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>AI编码不是实际编码更有用的技能</title>
      <link>https://www.reddit.com/r/ArtificialInteligence/comments/1mwwqu9/ai_coding_is_not_a_more_useful_skills_than_actual/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  似乎这些论坛完全是关于吹嘘自己的AI工作流程多么复杂的人。关于他们的上下文如何提出Claude代码的合法技能。我就像还好吗？这比在C ++中学习游戏开发或编写数据库或学习内存管理更为复杂吗？ 这等同于设置开发人员已经知道该怎么做的开发工作流程和环境。设置Claude代码是否比通过自定义配置和工作流程设置Neovim更复杂？可能不是。 那么，您知道克劳德代码是这项疯狂的新技能，从本质上讲，您只需在整个地方都有文本文件。归根结底，它只是以非确定性的方式生成了一堆代码。或者充其量只是成为一种花式的自动完成，因为您已经限制了模型如此之多，以至于无论如何您还是只要自己编码所有内容。 ，看来只有非编码器似乎只包含Vibe编码。同时，我去了与开发人员有关的论坛，并且有清理错误的恐怖故事。 这是关于LLM的事情，没人想承认： 它们是不可预测的，永远不会预测的。 这就是为什么我只能研究它们的原因。我没有用它们实际做实际的工作。因为工作需要上下文并试图使LLMS上下文意识到上游正在与上游进行战斗。 在短上下文中，窗口很难增加，因为 增加上下文窗口具有二次复杂性。它需要更多的矩阵乘法。 有优化，但它们具有稀疏注意的缺点。但是它的精度较小。  llms仅限于其数学。要通过上下文窗口绕过问题，您需要完全丢弃注意力机制 这对开发意味着什么？根据代码库的复杂性，LLM的性能越来越差。而且，您向LLM的外包代码越多，您对架构的介绍的黑匣子行为 因此，所有这些工作和“ AI技能”的范围比仅仅了解代码更糟糕。由于LLM的基本数学，情况将会变得更糟。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/golanglinuxguru1979       [links]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/ArtificialInteligence/comments/1mwwqu9/ai_coding_is_not_a_more_useful_skills_than_actual/</guid>
      <pubDate>Fri, 22 Aug 2025 04:17:13 GMT</pubDate>
    </item>
    </channel>
</rss>